{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer（优化器）\n",
    "## 优化器的目的\n",
    "   **优化器是为了一步一步走到代价函数的最小值那里**\n",
    "![6_1](image/class6_1.png)\n",
    "## 各种优化器\n",
    "1. 标准梯度下降法\n",
    "    - 标准梯度下降先计算所有样本汇总误差，然后根据总误差来更新权值\n",
    "    `tf.train.GradientDescentOptimizer`\n",
    "2. 随机梯度下降法\n",
    "    - 随机梯度下降随机抽取一个样本来计算误差，然后更新权值\n",
    "    - 随机梯度下降的公式\n",
    " ![6-2](image/class6_2.png)\n",
    "3. 批量梯度下降法\n",
    "    - 批量梯度下降是一种折中的方案，从总样本中选取一个批次（比如一共有10000个样本，随机选取100个样本作为个batch），然后计算这个batch的总误差，根据总误差来更新权值\n",
    " \n",
    " ![6-3](image/class6_3.png)\n",
    " ![6-4](image/class6_4.png)\n",
    " ![6-5](image/class6_5.png)\n",
    " ![6-6](image/class6_6.png)\n",
    " ![6-7](image/class6_7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/yuejinxiong/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/yuejinxiong/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/yuejinxiong/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/yuejinxiong/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "第1次迭代的测试集准确率: 0.9189\n",
      "第2次迭代的测试集准确率: 0.9242\n",
      "第3次迭代的测试集准确率: 0.9253\n",
      "第4次迭代的测试集准确率: 0.9294\n",
      "第5次迭代的测试集准确率: 0.9243\n",
      "第6次迭代的测试集准确率: 0.9312\n",
      "第7次迭代的测试集准确率: 0.9314\n",
      "第8次迭代的测试集准确率: 0.9306\n",
      "第9次迭代的测试集准确率: 0.9314\n",
      "第10次迭代的测试集准确率: 0.9301\n",
      "第11次迭代的测试集准确率: 0.9289\n",
      "第12次迭代的测试集准确率: 0.9317\n",
      "第13次迭代的测试集准确率: 0.9324\n",
      "第14次迭代的测试集准确率: 0.9326\n",
      "第15次迭代的测试集准确率: 0.9323\n",
      "第16次迭代的测试集准确率: 0.9313\n",
      "第17次迭代的测试集准确率: 0.9316\n",
      "第18次迭代的测试集准确率: 0.932\n",
      "第19次迭代的测试集准确率: 0.9308\n",
      "第20次迭代的测试集准确率: 0.9324\n",
      "第21次迭代的测试集准确率: 0.9309\n"
     ]
    }
   ],
   "source": [
    "# 优化器的使用案例\n",
    "# 手写数字识别案例(线性分类器)\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# 载入数据集。第一个参数是路径名，第二个是采用的编码\n",
    "mnist = input_data.read_data_sets(\"/Users/yuejinxiong/MNIST_data\",one_hot=True)\n",
    "\n",
    "# 每个批次的大小(每次放入100张图片去训练)\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "y = tf.placeholder(tf.float32, [None,10])\n",
    "# keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x, W)+b)\n",
    "\n",
    "\n",
    "# # 第一层神经元\n",
    "# # 用截断的正态分布来初始化,其标准差是0.1\n",
    "# W1 = tf.Variable(tf.truncated_normal([784, 2000],stddev=0.1))\n",
    "# # 偏置值初始化为0.1\n",
    "# b1 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "# L1 = tf.nn.tanh(tf.matmul(x, W1)+b1)\n",
    "# # 调用封装好的dropout函数,其中的keep_prob参数就是设置让 百分之几的神经元工作,如：0.5就是50%的神经元工作\n",
    "# L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# # 第二层神经元\n",
    "# W2 = tf.Variable(tf.truncated_normal([2000, 2000],stddev=0.1))\n",
    "# b2 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "# L2 = tf.nn.tanh(tf.matmul(L1_drop, W2)+b2)\n",
    "# L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# # 第三层神经元(用这么多层神经元,每层这么多神经元,就是为了模拟过拟合)\n",
    "# W3 = tf.Variable(tf.truncated_normal([2000, 1000],stddev=0.1))\n",
    "# b3 = tf.Variable(tf.zeros([1000])+0.1)\n",
    "# L3 = tf.nn.tanh(tf.matmul(L2_drop, W3)+b3)\n",
    "# L3_drop = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "# # 第四层神经元(用这么多层神经元,每层这么多神经元,就是为了模拟过拟合)\n",
    "# W4 = tf.Variable(tf.truncated_normal([1000, 10],stddev=0.1))\n",
    "# b4 = tf.Variable(tf.zeros([10])+0.1)\n",
    "# prediction = tf.nn.softmax(tf.matmul(L3_drop, W4)+b4)\n",
    "\n",
    "\n",
    "# 增加了隐藏层之后,参数变多了,更加难以收敛,如果想得到好的结果,必须要迭代很多次\n",
    "# 隐藏层的神经元个数最好是2^n个\n",
    "\n",
    "# 输入层与隐藏层之间的权重和偏置的初始化 隐藏层有5个神经元\n",
    "# Weight_L1 = tf.Variable(tf.random_normal([784, 512]))\n",
    "# biases_L1 = tf.Variable(tf.zeros([1, 512]))\n",
    "# Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "# L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "\n",
    "# 隐藏层和输出层之间权重和偏置的初始化\n",
    "# Weight_L2 = tf.Variable(tf.random_normal([512, 10]))\n",
    "# biases_L2 = tf.Variable(tf.zeros([1, 10]))\n",
    "# Wx_plus_b_L2 = tf.matmul(L1, Weight_L2) + biases_L2\n",
    "# prediction = tf.nn.softmax(Wx_plus_b_L2)\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "# 现在改为对数似然函数来优化\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "# 随机梯度下降法\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# 此次不在使用普通的优化器而转用一些别的优化器(Adam) 10^-2\n",
    "# 发现学习率虽然比上面的0.2更小，但是收敛速度却更快了\n",
    "train_step = tf.train.AdamOptimizer(1e-2).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 这里求预测的正确数、结果存在一个布尔型的列表中\n",
    "# equal(num1, num2)比较两个参数的大小是否一样,返回值为True或False\n",
    "# arg_max() 求y(一维张量)这个结果最大的值的索引位置(如果标签值和预测值最大的值在同一个位置，则说明该条正确) 1表示按行查找\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "\n",
    "# 求准确率, cast()将布尔类型的列表中的元素转成32位的浮点类型,然后再求一个平均值(true=1.0 false=0)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # 所有数据迭代21次\n",
    "    for epoch in range(21):\n",
    "        # 一次迭代中迭代计算好的数量个批次\n",
    "        for batch in range(n_batch):\n",
    "            # 获取100个批次保存在里面,数据保存在batch_xs中，标签保存在batchys中\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # 将keep_prob 分别设置为1.0（所有神经元均工作） 和0.5等等进行比较\n",
    "            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})\n",
    "        # 训练一个周期后,可以查看其准确率的变化\n",
    "        # 查看训练集的准确率\n",
    "        # train_acc = sess.run(accuracy, feed_dict={x:mnist.train.images, y:mnist.train.labels, keep_prob:1.0})\n",
    "        # 查看测试集的准确率\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "        print(\"第\"+str(epoch+1)+\"次迭代的测试集准确率: \"+str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结\n",
    "## 目前的几种提高模型的准确率和训练速度的方法\n",
    "1. 更改损失函数，原先使用二次代价函数，对于softmax或者sigmoid可以使用交叉熵来作为代价函数\n",
    "2. 权值矩阵W一般最好初始化为截断的随机正态分布；偏置b一般可以初始化为0.1，而不是用0来初始化\n",
    "3. 适当增加隐藏层可以更好的拟合函数，提高模型的准确率\n",
    "4. 更改激活函数，找到适合模型的激活函数：sigmoid、tanh或者Relu等\n",
    "5. 防止过拟合问题，在训练时可以采用dropout来随机使一些神经元失去活性\n",
    "6. 优化器不再使用随机梯度下降而改用其他的，明显提升梯度下降的速度；优化器的学习率也可以适当调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用上述小结中的方法来优化上面的模型使其准确率达到98%以上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/yuejinxiong/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/yuejinxiong/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/yuejinxiong/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/yuejinxiong/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "第1次迭代的测试集准确率: 0.9387;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第2次迭代的测试集准确率: 0.9473;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第3次迭代的测试集准确率: 0.9478;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第4次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第5次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第6次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第7次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第8次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第9次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第10次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第11次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第12次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第13次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第14次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第15次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第16次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第17次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第18次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第19次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第20次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n",
      "第21次迭代的测试集准确率: 0.9479;学习率为：<tf.Variable 'Variable_11:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# 综合优化案例\n",
    "# 手写数字识别案例(线性分类器)\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# 载入数据集。第一个参数是路径名，第二个是采用的编码\n",
    "mnist = input_data.read_data_sets(\"/Users/yuejinxiong/MNIST_data\",one_hot=True)\n",
    "\n",
    "# 每个批次的大小(每次放入100张图片去训练)\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "y = tf.placeholder(tf.float32, [None,10])\n",
    "# keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.Variable(0.001, dtype=tf.float32)\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "# W = tf.Variable(tf.zeros([784, 10]))\n",
    "# b = tf.Variable(tf.zeros([10]))\n",
    "# prediction = tf.nn.softmax(tf.matmul(x, W)+b)\n",
    "\n",
    "\n",
    "# # 第一层神经元\n",
    "# # 用截断的正态分布来初始化,其标准差是0.1\n",
    "# W1 = tf.Variable(tf.truncated_normal([784, 2000],stddev=0.1))\n",
    "# # 偏置值初始化为0.1\n",
    "# b1 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "# L1 = tf.nn.tanh(tf.matmul(x, W1)+b1)\n",
    "# # 调用封装好的dropout函数,其中的keep_prob参数就是设置让 百分之几的神经元工作,如：0.5就是50%的神经元工作\n",
    "# L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# # 第二层神经元\n",
    "# W2 = tf.Variable(tf.truncated_normal([2000, 2000],stddev=0.1))\n",
    "# b2 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "# L2 = tf.nn.tanh(tf.matmul(L1_drop, W2)+b2)\n",
    "# L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# # 第三层神经元(用这么多层神经元,每层这么多神经元,就是为了模拟过拟合)\n",
    "# W3 = tf.Variable(tf.truncated_normal([2000, 1000],stddev=0.1))\n",
    "# b3 = tf.Variable(tf.zeros([1000])+0.1)\n",
    "# L3 = tf.nn.tanh(tf.matmul(L2_drop, W3)+b3)\n",
    "# L3_drop = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "# # 第四层神经元(用这么多层神经元,每层这么多神经元,就是为了模拟过拟合)\n",
    "# W4 = tf.Variable(tf.truncated_normal([1000, 10],stddev=0.1))\n",
    "# b4 = tf.Variable(tf.zeros([10])+0.1)\n",
    "# prediction = tf.nn.softmax(tf.matmul(L3_drop, W4)+b4)\n",
    "\n",
    "\n",
    "# 增加了隐藏层之后,参数变多了,更加难以收敛,如果想得到好的结果,必须要迭代很多次\n",
    "# 隐藏层的神经元个数最好是2^n个\n",
    "\n",
    "# 优化1:增加一层隐藏层，隐藏层神经元1024个；优化2:初始化权重矩阵W和偏置b时分别采用截断的随机正态分布和0.1\n",
    "# 输入层与隐藏层之间的权重和偏置的初始化 隐藏层有5个神经元\n",
    "Weight_L1 = tf.Variable(tf.truncated_normal([784, 1024], stddev=0.1))\n",
    "biases_L1 = tf.Variable(tf.zeros([1, 1024]) + 0.1)\n",
    "Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1\n",
    "L1 = tf.nn.tanh(Wx_plus_b_L1)\n",
    "\n",
    "# 隐藏层和输出层之间权重和偏置的初始化\n",
    "Weight_L2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
    "biases_L2 = tf.Variable(tf.zeros([1, 10]) + 0.1)\n",
    "Wx_plus_b_L2 = tf.matmul(L1, Weight_L2) + biases_L2\n",
    "prediction = tf.nn.softmax(Wx_plus_b_L2)\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "# 优化3: 现在改为softmax的专用的对数似然函数来优化\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "\n",
    "# 随机梯度下降法\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# 优化4:此次不在使用普通的优化器而转用一些别的优化器(Adam) 10^-2\n",
    "# 发现学习率虽然比上面的0.2更小，但是收敛速度却更快了 lr-->1e-3\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 这里求预测的正确数、结果存在一个布尔型的列表中\n",
    "# equal(num1, num2)比较两个参数的大小是否一样,返回值为True或False\n",
    "# arg_max() 求y(一维张量)这个结果最大的值的索引位置(如果标签值和预测值最大的值在同一个位置，则说明该条正确) 1表示按行查找\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "\n",
    "# 求准确率, cast()将布尔类型的列表中的元素转成32位的浮点类型,然后再求一个平均值(true=1.0 false=0)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # 所有数据迭代21次\n",
    "    for epoch in range(21):\n",
    "        # 优化5:初始的学习率为0.001，每迭代一个周期，就将学习率调低\n",
    "        # sess.run(tf.assign(lr, 0.001*(0.095 ** epoch)))\n",
    "        # 一次迭代中迭代计算好的数量个批次\n",
    "        for batch in range(n_batch):\n",
    "            # 获取100个批次保存在里面,数据保存在batch_xs中，标签保存在batchys中\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # 将keep_prob 分别设置为1.0（所有神经元均工作） 和0.5等等进行比较\n",
    "            sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})\n",
    "        # 训练一个周期后,可以查看其准确率的变化\n",
    "        # 查看训练集的准确率\n",
    "        # train_acc = sess.run(accuracy, feed_dict={x:mnist.train.images, y:mnist.train.labels, keep_prob:1.0})\n",
    "        # 查看测试集的准确率\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "        print(\"第\"+str(epoch+1)+\"次迭代的测试集准确率: \"+str(test_acc)+\";学习率为：**\")\n",
    "        # +str(sess.run(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-7for learn",
   "language": "python",
   "name": "py3-7forlearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
